<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-12-11 Fri 14:35 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Assignment 3</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Hasan Shahoud" />
<meta name="description" content="Activity Recognition"
 />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>

<script type="text/javascript" src="https://orgmode.org/org-info.js">
/**
 *
 * @source: https://orgmode.org/org-info.js
 *
 * @licstart  The following is the entire license notice for the
 *  JavaScript code in https://orgmode.org/org-info.js.
 *
 * Copyright (C) 2012-2020 Free Software Foundation, Inc.
 *
 *
 * The JavaScript code in this tag is free software: you can
 * redistribute it and/or modify it under the terms of the GNU
 * General Public License (GNU GPL) as published by the Free Software
 * Foundation, either version 3 of the License, or (at your option)
 * any later version.  The code is distributed WITHOUT ANY WARRANTY;
 * without even the implied warranty of MERCHANTABILITY or FITNESS
 * FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.
 *
 * As additional permission under GNU GPL version 3 section 7, you
 * may distribute non-source (e.g., minimized or compacted) forms of
 * that code without the copy of the GNU GPL normally required by
 * section 4, provided you include this license notice and a URL
 * through which recipients can access the Corresponding Source.
 *
 * @licend  The above is the entire license notice
 * for the JavaScript code in https://orgmode.org/org-info.js.
 *
 */
</script>

<script type="text/javascript">

/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/

<!--/*--><![CDATA[/*><!--*/
org_html_manager.set("TOC_DEPTH", "3");
org_html_manager.set("LINK_HOME", "");
org_html_manager.set("LINK_UP", "../../main.html");
org_html_manager.set("LOCAL_TOC", "1");
org_html_manager.set("VIEW_BUTTONS", "0");
org_html_manager.set("MOUSE_HINT", "underline");
org_html_manager.set("FIXED_TOC", "0");
org_html_manager.set("TOC", "1");
org_html_manager.set("VIEW", "info");
org_html_manager.setup();  // activate after the parameters are set
/*]]>*///-->
</script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="../../main.html"> UP </a>
 |
 <a accesskey="H" href=""> HOME </a>
</div><div id="content">
<h1 class="title">Assignment 3</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgc644374">1. Artificial Intelligence: activity recognition and its benefits and usage</a></li>
<li><a href="#org8b8c1e2">2. References</a></li>
</ul>
</div>
</div>


<div id="outline-container-orgc644374" class="outline-2">
<h2 id="orgc644374"><span class="section-number-2">1</span> Artificial Intelligence: activity recognition and its benefits and usage</h2>
<div class="outline-text-2" id="text-1">
<p>
Different Artificial Intelligence and Machine Learning's approaches are exploited with the help of particular (censoring) devices such as the ubiquitous smartphones or wearable sensors, to classify and analyze data, and subsequently produce helpful functions; such as Human Activity Recognition (HAR). A myriad of research have been conducted on monitoring HAR, which is advantageous in various areas: habit tracking or health and medicine. For example, in study [1], 29 users placed a smartphone in their pocket. The device helped collect and categorize data to predict users' activities such as: walking, standing, sitting, climbing and jogging. In another research, users wore cheap, wireless sensors that were waist-mounted [2]. These sensors would transmit data to an external storage unit, and by means of Convolutional Neural Networks (CNN), the signals were analyzed and HAR of the elderlies was eventually obtained. Due to privacy concerns, quicker data processing and easier usability systems, a different approach [3] was proposed to build a dialogue-based annotation system for HAR, which allowed users to record their activities by voice-based or keyboard input. This system would help, for instance, a nurse record activities while taking care of patients. In the following sections, three different HAR approaches and their mechanisms and benefits will be presented.
</p>


<p>
The first HAR approach to be discussed, has focused on waist-mounted wearable sensors for analyzing and obtaining data [1]. These types of sensors are wired with accelerometers and gyroscopes, which can provide accurate hip motion signals. These signals are difficult to generate otherwise. The interpretation of these signals resulted from a deep learning paradigm: CNN. CNN is a method that can learn complex patterns. The benefit of using CNN lies in its ability to generate these data patterns without requiring hand-crafted features extraction, which can damage both the recognition speed and accuracy. Two sets of CNN were used, which received the inputs from the activity signals (or images) and the outputs were 8 classes of the human activities in the training data. The outputs were fused together using soft voting technique, which would choose the class with the highest vote. Moreover, 15 individuals were selected in a cross-subjects test design, of which 8 were men and 7 women, to perform certain activities, like walking, jumping or running. For training the CNN, the activity data of 12 individuals were used, while the data for the other 3, were reserved for testing. This system yielded an overall accuracy of 87%.
</p>


<p>
Kwapisz, Weiss and Moore [2] introduced another approach to attain HAR. They, on the other hand, exploited phone-based accelerometers and categorized accelerometer data from twenty-nine users, while performing activities like: walking, jogging, climbing stairs, sitting and standing, since these activities are performed regularly. They relied on a user interface for data collection, which they quality-checked by their WISDM project [4]. A supervised learning task used 20 samples per second to generate a set of features each contained 200 raw accelerometer readings, and each reading has x, y and z values; horizontal, upward and downward, and forward movements respectively. Subsequently, their results represented distinctive patterns for standing and sitting, and a periodic behavior among the other activities. Moreover, they achieved above 90% accuracy for walking and jogging. However, the upward and downward movements were combined into one activity, because of the prediction errors resulting from the climbing stairs movements. As a result, an overall accuracy of 91.7% was obtained with the help of the Multiplayer Perception algorithm. Finally, they suggested that their approach could help in many areas such as: redirecting calls to voicemail while the users perform certain exercises, or suggesting an appropriate action to assist the user to improve accordingly, for example, alert the user if they consumed more calories than advised. 
</p>


<p>
The final approach [3], on the contrary, aims at creating a dialogue-based annotation system (DBAS) that is both simple to use and interactive. This system functions on Google Assistant (GA) to collect activity data from the users. It also uses the open-source library Dialogflow. Dialogflow provides a human conversational functionality to understand users' utterances. The combination of both GA and Dialogflow is an optimal choice, because DBAS is a fram-based architecture, which focuses on task-oriented dialogue and short conversations. Although this system was intended to be simple to use, it does encounter some difficulties, for example: if the user doesn't speak English properly, it might cause errors for the system and annoyance for the user, or if the user has their smartphone in their pocket, they are required to unlock the screen in order to interact with the system. The latter point is crucial, because the original intention of the system was to provide a better usability and without any initiation steps. For example, if a physiotherapist is helping a patient, it is inefficient and obtuse to use the system. However, this can be solved by implementing the system with smart assistants, for example Google Home. 
</p>


<p>
In conclusion, and with regard to the different approaches that I've presented, different sensors are used in order to produce an accurate HAR. I've also listed their methods for measuring HAR as well as some use cases. Recognizing human activities can be essential for many applications. As can be seen above, these HAR systems can help, for example, 1 suggest an action that the user should consider or must take, which is based on the activities in which  the user has been involved, 2 a nurse while she is taking care of a patient, or 3 even make predictions about the physical health and further consequences of certain movements. Also, interactive systems can be used to measure HAR. These systems have the benefit of being secure, simple and on demand. However, they do not come without caveats such as less accurate results since users are forced to use extra brain efforts; i.e. remembering time points and registering them. But in the future, possible combinations of various HAR systems can tackle such problems. Overall, HAR systems have many prospects and are useful to both understand human behavior and help assist people, from different categories, and inform them about health related issues and possible outcomes based on specific and predicted activities. 
</p>
</div>
</div>



<div id="outline-container-org8b8c1e2" class="outline-2">
<h2 id="org8b8c1e2"><span class="section-number-2">2</span> References</h2>
<div class="outline-text-2" id="text-2">
<p>
[1] J. R. Kwapisz, G. M. Weiss, and S. A. Moore, "Activity recognition using cell phone accelerometers." <i>SIGKDD Explor. Newsl. 12, vol. 2, pp. 74–82</i>, March 2011. [Online]. Available: <a href="https://doi.org/10.1145/1964897.1964918">https://doi.org/10.1145/1964897.1964918</a>
</p>

<p>
[2] I. A. Lawal and S. Bano, "Deep human activity recognition using wearable sensors." <i>In Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments (PETRA ’19). Association for Computing Machinery, New York, NY, USA, pp. 45–48</i>. 2019. [Online]. Available: <a href="https://doi.org/10.1145/3316782.3321538">https://doi.org/10.1145/3316782.3321538</a>
</p>


<p>
[3] T. Mairittha, N. Mairittha, and S. Inoue, "A dialogue-based annotation for activity recognition." <i>In Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (UbiComp/ISWC ’19 Adjunct). Association for Computing Machinery, New York, NY, USA, pp. 768–773</i>. [Online]. Available: <a href="https://doi.org/10.1145/3341162.3345594">https://doi.org/10.1145/3341162.3345594</a>
</p>

<p>
[4] G. M. Weiss, "WISDM (Wireless Sensor Data Mining) Project." <i>Fordham University, Department of Computer and Information Science 2010.</i>, [Online]. Available: <a href="http://storm.cis.fordham.edu/~gweiss/wisdm/">http://storm.cis.fordham.edu/~gweiss/wisdm/</a>
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Hasan Shahoud</p>
<p class="date">Created: 2020-12-11 Fri 14:35</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
<script type="text/javascript" src="../imageModal.js"></script>